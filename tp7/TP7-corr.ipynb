{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import to_timeseries_input, splitDate, Trainer, PredictReconstruction, PlotResult\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Recurrent Neural Networks\n",
    "\n",
    "In this TP, we will introduce Recurrent Neural Networks (RNNs) and demonstrate how to use them to model time-series data. Specifically, we will be working with the CUBEMS datasets, which contains measurements of temperature, humidity, and light intensity in a 7 floors buildings in Bangkok, Thailand. We will use only a small portion of the data consisting of measurement from 1 zones on the 7th floors. You can explore the dataset by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data.csv\"\n",
    "data = pd.read_csv(path)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.set_index('Date')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this TP, we will focus on predicting the humidity measurement. We will split the dataset to train and test by taking a date and limit the test data to only 1 months for visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns to use\n",
    "col = [\"humidity\"] # \n",
    "data = data[[*col]]\n",
    "\n",
    "date_limit = pd.to_datetime(\"2019-09-20\")\n",
    "date_limit2 = pd.to_datetime(\"2019-09-30\")\n",
    "train_date, test_date = splitDate(data.index, date_limit)\n",
    "train_data = data.loc[data.index < date_limit][col].values\n",
    "test_data = data.loc[(data.index >= date_limit) & (data.index < date_limit2)][col].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To time series input\n",
    "lookback = 24  \n",
    "lookahead = 1 \n",
    "batch_size = 128\n",
    "\n",
    "# Train and validation data\n",
    "train_input, train_output = to_timeseries_input(train_data, lookback, lookahead) \n",
    "trainx, validx, trainy, validy = train_test_split(train_input, train_output, test_size=0.3, shuffle=False) \n",
    "\n",
    "train_tensor = TensorDataset(torch.from_numpy(trainx).float(), torch.from_numpy(trainy).float()) \n",
    "val_tensor = TensorDataset(torch.from_numpy(validx).float(), torch.from_numpy(validy).float()) \n",
    "\n",
    "train_loader = DataLoader(dataset=train_tensor, batch_size=batch_size, shuffle=False) \n",
    "val_loader = DataLoader(dataset=val_tensor, batch_size=batch_size, shuffle=False)  \n",
    "\n",
    "\n",
    "# Test data\n",
    "test_input, test_output = to_timeseries_input(test_data, lookback, lookahead) \n",
    "test_tensor = TensorDataset(torch.from_numpy(test_input).float(), torch.from_numpy(test_output).float()) \n",
    "test_loader = DataLoader(dataset=test_tensor, batch_size=batch_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "To build a simple Recurrent Neural Network (RNN) model, we will start with an input layer that takes a sequence of humidity measurements as input. The input tensor has a shape of `(batch_size, T, 1)`, where `batch_size` is the number of sequences in the batch, `T` is the length of each sequence, and `1` is the size of the input vector at each time step.\n",
    "\n",
    "Next, we will add a hidden layer that is an RNN layer with `hidden_size` units. The hidden layer takes the input sequence and maintains a hidden state vector of size `hidden_size` at each time step. The output of the hidden layer has shape `(batch_size, T, hidden_size)`.\n",
    "\n",
    "Finally, we will add an output layer that is a fully connected layer with 1 unit. The output layer takes the output vector from the hidden layer and computes the predicted value for `1` timesteps. The output tensor has shape `(batch_size, 1, 1)`.\n",
    "\n",
    "#### Your job: \n",
    "Complete the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16 \n",
    "in_features = 1 \n",
    "out_features = 1 \n",
    "output_length = lookahead\n",
    "epochs = 20\n",
    "patience = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size, out_features, out_len):\n",
    "        \"\"\"\n",
    "        Initialize the SimpleRNN model.\n",
    "        Args:\n",
    "            in_features (int): The number of input features.\n",
    "            hidden_size (int): The number of features in the hidden state.\n",
    "            out_features (int): The number of output features.\n",
    "            out_len (int): The length of the output sequence.\n",
    "        \"\"\"\n",
    "        super(SimpleRNN,self).__init__()\n",
    "        self.out_len = out_len\n",
    "        self.rnn = nn.RNN(in_features, hidden_size, batch_first=True, dropout=0.3) \n",
    "        self.fc = nn.Linear(hidden_size, out_features) \n",
    "    def forward(self,x):\n",
    "        out, h_en = self.rnn(x) \n",
    "        out = self.fc(out[:, -self.out_len:, :]) \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(in_features, hidden_size, out_features, output_length).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters()) \n",
    "criterion = nn.MSELoss() \n",
    "# Train Loop\n",
    "trainer = Trainer(model, train_loader=train_loader,  \n",
    "                  val_loader=val_loader, \n",
    "                  test_loader=test_loader, \n",
    "                  criterion = criterion, \n",
    "                  optimizer = optimizer, \n",
    "                  device = device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_metric, val_loss, val_metrics = trainer.train(epochs=epochs, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_metrics(train_loss, val_loss, train_metric, val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rnn, pred_rnn = PredictReconstruction(model, test_loader, lookahead,device)\n",
    "PlotResult(target_rnn, pred_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Long Short Term Memory (LSTM)\n",
    "\n",
    "In this section, we are going to explore another type of RNN model, namely Long Short Term Memory (LSTM). Unlike traditional Recurrent Neural Networks (RNNs), which struggle to maintain and utilize information from earlier steps in long sequences due to the vanishing gradient problem, LSTMs are designed to effectively capture long-term dependencies in sequence data.\n",
    "\n",
    "**Your task**: \n",
    "\n",
    "Complete the model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size , out_features, out_len):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.out_len = out_len\n",
    "        self.lstm = nn.LSTM(in_features, hidden_size, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_size, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #TODO Complet the forward pass\n",
    "        out, (h_en,c_en) = self.lstm(x)\n",
    "        out = self.fc(out[:,-self.out_len:,:]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = SimpleLSTM(in_features, hidden_size, out_features, output_length).to(device)\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "trainer = Trainer(lstm_model, train_loader=train_loader, \n",
    "                  val_loader=val_loader, \n",
    "                  test_loader=test_loader,\n",
    "                  criterion = criterion, \n",
    "                  optimizer = optimizer,\n",
    "                  device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_metric, val_loss, val_metrics = trainer.train(epochs=epochs, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_metrics(train_loss, val_loss, train_metric, val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lstm, pred_lstm = PredictReconstruction(lstm_model, test_loader, lookahead,device)\n",
    "PlotResult(target_lstm, pred_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many-to-Many RNN Architecture\n",
    "\n",
    "The existing code provides a one-step prediction for each time-series. We will now modify this to explore a many-to-many RNN architecture.\n",
    "\n",
    "Your task is to adapt the existing cells for many-to-many prediction. Here are some suggestions to guide you:\n",
    "\n",
    "1. **Data Splitting**: Begin by adjusting the data split. Experiment with setting the `lookback` equal to `lookahead`, and then try making `lookback` greater than `lookahead`.\n",
    "\n",
    "2. **Modify the RNN and LSTM Models**: Adjust the SimpleRNN and Simple LSTM models to accommodate the many-to-many setting.\n",
    "\n",
    "3. **Training**: Train the model with the new many-to-many setting.\n",
    "\n",
    "After completing these steps, observe the prediction quality. How does the many-to-many model compare to the original many-to-one model in terms of prediction quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
