{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082dc9ba",
   "metadata": {},
   "source": [
    "## TP1: Performance evaluation\n",
    "\n",
    "In this practical session you  are going to manipulate the sklearn library.\n",
    "The main goal is to analyze the performance of several methods on a binary classification task.\n",
    "You will experiment with different metrics that can be used to evaluate the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e773b646",
   "metadata": {},
   "source": [
    "## Dataset loading\n",
    "\n",
    "The breast cancer dataset is a binary classification dataset.\n",
    "We are first interested to know the input dimensionality, the number of samples, the number of positive samples and the ratio of positive samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9894d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension:  30\n",
      "Nb of samples in the full dataset:  569\n",
      "Nb of positive samples 357\n",
      "Positive Ratio: 0.6274165202108963\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "input_dimension = X.shape[1]\n",
    "nb_samples = X.shape[0]\n",
    "\n",
    "from numpy import sum\n",
    "nb_positives = sum(y == 1)\n",
    "positive_ratio = nb_positives / nb_samples\n",
    " \n",
    "print(\"Input dimension: \", input_dimension)\n",
    "print(\"Nb of samples in the full dataset: \", nb_samples)\n",
    "print(\"Nb of positive samples\", nb_positives)\n",
    "print(\"Positive Ratio:\", positive_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf2efec",
   "metadata": {},
   "source": [
    "*Q*: What is the sample dimension? \n",
    "\n",
    "*Q*: How many samples does the dataset contain?\n",
    "\n",
    "*Q*: What is the positive ratio in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3808f",
   "metadata": {},
   "source": [
    "### Dataset split\n",
    "Now we are going to perform a split of the data for the evaluation. Use the train_test_split function with a test_size parameter of 0.3.\n",
    "\n",
    "Then compute the number of samples in the train and test set, as well as their positive ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397a6b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb samples train 398\n",
      "Nb samples test 171\n",
      "Nb of positive train samples 244\n",
      "Nb of positive test samples 113\n",
      "Train Positive Ratio: 0.6130653266331658\n",
      "Test Positive Ratio: 0.6608187134502924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "nb_samples_train = X_train.shape[0] \n",
    "nb_samples_test = X_test.shape[0]\n",
    "\n",
    "nb_train_positives = sum(y_train == 1)\n",
    "nb_test_positives = sum(y_test == 1)\n",
    "\n",
    "train_positive_ratio = nb_train_positives / nb_samples_train\n",
    "test_positive_ratio = nb_test_positives / nb_samples_test\n",
    "\n",
    "print(\"Nb samples train\", nb_samples_train)\n",
    "print(\"Nb samples test\", nb_samples_test)\n",
    "print(\"Nb of positive train samples\", nb_train_positives)\n",
    "print(\"Nb of positive test samples\", nb_test_positives)\n",
    "print(\"Train Positive Ratio:\", train_positive_ratio)\n",
    "print(\"Test Positive Ratio:\", test_positive_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe93cb",
   "metadata": {},
   "source": [
    "Q: How many samples do you have in the training set?\n",
    "\n",
    "Q: How many samples in the test set?\n",
    "\n",
    "Q: Are the train and test positive ratios similar ? (answer will differ depending on the random state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076da3ce",
   "metadata": {},
   "source": [
    "## Train and evaluate methods\n",
    "\n",
    "We are now going to train some classification methods on the train data.\n",
    "For today we are going to use Nearest Neighbor, Logistic Regression and a Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b9e1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf_nn = KNeighborsClassifier(n_neighbors=3)\n",
    "clf_tree = DecisionTreeClassifier();\n",
    "clf_log = LogisticRegression(max_iter=5000);\n",
    "\n",
    "clf_nn.fit(X_train, y_train); \n",
    "clf_tree.fit(X_train, y_train);\n",
    "clf_log.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a822d74d",
   "metadata": {},
   "source": [
    "Once the classifiers trained, they are ready to ```predict```. Use the prediction functions to obtain the predictions on the test data.\n",
    "\n",
    "Sometimes it is interesting to not only look at the final prediction, but to have a look at the predicted probability. You can use the ```predict_proba``` function. From its output, for all predictions, you can keep the probability of the second class (having cancer). This is effectively performed with the python selector [:,1].\n",
    "\n",
    "Q: For the nearest neighbor method, print the difference between the predictions and the probabilities. As you will see they are not zero. Predictions are binary, while probabilities are floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341f7610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of non-zero differences for Nearest Neighbor: 14.62%\n",
      "Percentage of non-zero differences for Decision Tree: 0.00%\n",
      "Percentage of non-zero differences for Logistic Regression: 100.00%\n"
     ]
    }
   ],
   "source": [
    "y_pred_nn = clf_nn.predict(X_test)\n",
    "y_pred_tree = clf_tree.predict(X_test)\n",
    "y_pred_log = clf_log.predict(X_test)\n",
    "\n",
    "y_proba_nn = clf_nn.predict_proba(X_test)[:,1]\n",
    "y_proba_tree = clf_tree.predict_proba(X_test)[:,1]\n",
    "y_proba_log = clf_log.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "# Compute the difference between prediction and probability for each classifier\n",
    "diff_nn = sum(y_pred_nn - y_proba_nn != 0)\n",
    "diff_tree = sum(y_pred_tree - y_proba_tree != 0)\n",
    "diff_log = sum(y_pred_log - y_proba_log !=0)\n",
    "\n",
    "\n",
    "# Compute the percentage of non-zero differences\n",
    "percentage_diff_nn = (diff_nn / len(y_pred_nn)) * 100\n",
    "percentage_diff_tree = (diff_tree / len(y_pred_tree)) * 100\n",
    "percentage_diff_log = (diff_log / len(y_pred_log)) * 100\n",
    "\n",
    "# Print the percentages\n",
    "print(f\"Percentage of differences for Nearest Neighbor: {percentage_diff_nn:.2f}%\")\n",
    "print(f\"Percentage of differences for Decision Tree: {percentage_diff_tree:.2f}%\")\n",
    "print(f\"Percentage of differences for Logistic Regression: {percentage_diff_log:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b3d22",
   "metadata": {},
   "source": [
    "To have a sense of the difficulty of the problem it is usually helpful to evaluate simple baselines.\n",
    "In this case we ask you to generate three baseline results:\n",
    "- The results of a random method: use ```random.ramdom(N)``` to generate N probabilities between 0 and 1, and use ```rint``` to round them to final binary predictions.\n",
    "- The results of a methods *always* classifying as \"there is cancer\", just use the ```ones``` method.\n",
    "- The results of a methods *never* classifying as \"there is cancer\", just use the ```zeros``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d515ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create baselines\n",
    "from numpy import random, zeros, ones, rint\n",
    "\n",
    "N = len(y_test)\n",
    "\n",
    "y_proba_random = random.random(N)\n",
    "y_pred_random = rint(y_proba_random)\n",
    "\n",
    "y_pred_zeros = ones(N)\n",
    "y_pred_ones = zeros(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156aa57a",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Now that we have predictions and probabilities of 6 methods we are going to compare them with standard metrics.\n",
    "\n",
    "First we want to compute their accuracy, precision, recall and f1_score. Use the ```accuracy_score```, ```precision_score```, ```recall_score``` and  ```f1_score``` to compute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb810ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors:\n",
      "\tAccuracy  0.9473684210526315\n",
      "\tPrecision  0.940677966101695\n",
      "\tRecall  0.9823008849557522\n",
      "\tF1 score 0.961038961038961\n",
      "Decision Tree:\n",
      "\tAccuracy  0.9298245614035088\n",
      "\tPrecision  0.9316239316239316\n",
      "\tRecall  0.9646017699115044\n",
      "\tF1 score 0.9478260869565217\n",
      "Logistic Regression:\n",
      "\tAccuracy  0.9532163742690059\n",
      "\tPrecision  0.9646017699115044\n",
      "\tRecall  0.9646017699115044\n",
      "\tF1 score 0.9646017699115044\n",
      "Random:\n",
      "\tAccuracy  0.5555555555555556\n",
      "\tPrecision  0.6989247311827957\n",
      "\tRecall  0.5752212389380531\n",
      "\tF1 score 0.6310679611650486\n",
      "Always:\n",
      "\tAccuracy  0.3391812865497076\n",
      "\tPrecision  0.0\n",
      "\tRecall  0.0\n",
      "\tF1 score 0.0\n",
      "Never:\n",
      "\tAccuracy  0.6608187134502924\n",
      "\tPrecision  0.6608187134502924\n",
      "\tRecall  1.0\n",
      "\tF1 score 0.795774647887324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from numpy import rint\n",
    "import numpy as np\n",
    "\n",
    "def printMetrics(method_title, test_data, pred_data):\n",
    "    print(method_title) \n",
    "    print(\"\\tAccuracy \", accuracy_score(test_data, pred_data)) \n",
    "    print(\"\\tPrecision \", precision_score(test_data, pred_data))\n",
    "    print(\"\\tRecall \", recall_score(test_data, pred_data))\n",
    "    print(\"\\tF1 score\", f1_score(test_data, pred_data))\n",
    "        \n",
    "printMetrics(\"Nearest Neighbors:\", y_test, y_pred_nn) \n",
    "printMetrics(\"Decision Tree:\", y_test, y_pred_tree) \n",
    "printMetrics(\"Logistic Regression:\", y_test, y_pred_log) \n",
    "printMetrics(\"Random:\", y_test, y_pred_random) \n",
    "printMetrics(\"Always:\", y_test, y_pred_ones) \n",
    "printMetrics(\"Never:\", y_test, y_pred_zeros) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc5b1c0",
   "metadata": {},
   "source": [
    "Q: What do you observe from these results?\n",
    "\n",
    "Q: Explain the value of each metrics\n",
    "\n",
    "Q: From these results, what is your initial ranking for each methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4f857",
   "metadata": {},
   "source": [
    "### The ROC curve\n",
    "\n",
    "To create a ROC curve we need first to compute the false positive rate and the true positive rate.\n",
    "For that we should use the probabilities (before binarization). Compute the  positive rates for all 6 methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdc18d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "false_positive_rate_nn, true_positive_rate_nn, _ = roc_curve(y_test, y_pred_nn)\n",
    "false_positive_rate_tree, true_positive_rate_tree, _ = roc_curve(y_test, y_pred_tree)\n",
    "false_positive_rate_log, true_positive_rate_log, _ = roc_curve(y_test, y_pred_log)\n",
    "false_positive_rate_random, true_positive_rate_random, _ = roc_curve(y_test, y_pred_random)\n",
    "false_positive_rate_zeros, true_positive_rate_zeros, _ = roc_curve(y_test, y_pred_zeros)\n",
    "false_positive_rate_ones, true_positive_rate_ones, _ = roc_curve(y_test, y_pred_ones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7cd02",
   "metadata": {},
   "source": [
    "To plot a ROC curve you need to plot the False positive rate against the True positive rate.\n",
    "Complete the function definition and give the proper arguments to do the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572dc5bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotROC(title): # TODO: add FP and TP\n",
    "    plt.subplots(1, figsize=(10,10))\n",
    "    plt.title('Receiver Operating Characteristic - ' + title)\n",
    "    plt.plot(false_positive_rate_nn, true_positive_rate_nn) \n",
    "    plt.plot([0, 1], ls=\"--\") # This draws the diagonal\n",
    "    plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\") # This draws the \"perfect\" score\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "plotROC(\"Nearest\"); # TODO add FP and TP\n",
    "plotROC(\"DecisionTree\"); # TODO add FP and TP\n",
    "plotROC(\"Logistic regression\"); # TODO add FP and TP\n",
    "plotROC(\"Random\"); # TODO add FP and TP\n",
    "plotROC(\"Always\"); # TODO add FP and TP\n",
    "plotROC(\"Never\"); # TODO add FP and TP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1718c4",
   "metadata": {},
   "source": [
    "Q: Compare the curves. What do you see?\n",
    "\n",
    "Q: Particularly, compare the \"Always cancer\" and \"Never cancer\" curves. Why do you get these curves? Discuss their similarities and or differences.\n",
    "\n",
    "Q: Would your preliminary ranking of the methods change after seeing these curves? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311cbfd",
   "metadata": {},
   "source": [
    "For a better comparison you can plot all curves in the same plot.\n",
    "Use the parameter ```label='Data label'``` in the plot function to know which one is each and the ```plot.legend(loc=\"lower right\")``` to show the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7cb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, figsize=(10,10))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot([0,1], label=\"Zeros\"); # TODO\n",
    "plt.plot([0,1],  label=\"Ones\"); # TODO\n",
    "plt.plot([0,1],  label=\"Tree\"); # TODO\n",
    "plt.plot([0,1],  label=\"Random\"); # TODO\n",
    "plt.plot([0,1],  label=\"Logistic\"); # TODO\n",
    "plt.plot([0,1],  label=\"Nearest\"); # TODO\n",
    "\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "# TODO add legend\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724255d",
   "metadata": {},
   "source": [
    "To compute the Roc Area Under Curve (AUC) score you can use the function ```roc_auc_score```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('roc_auc_score for Nearest: ', 0) # TODO\n",
    "print('roc_auc_score for DecisionTree: ', 0) # TODO\n",
    "print('roc_auc_score for Logistic Regression: ', 0) # TODO\n",
    "print('roc_auc_score for Random Guessing: ', 0) # TODO\n",
    "print('roc_auc_score for Always No Cancer: ', 0) # TODO\n",
    "print('roc_auc_score for Always Cancer: ', 0) # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736821e8",
   "metadata": {},
   "source": [
    "Q: Do you have a strong preference on which method you would use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663a5bf",
   "metadata": {},
   "source": [
    "### Rerunning the process\n",
    "\n",
    "Save the current results (screenshot or write down the roc auc scores), then go back to the beginning of the notebook and rerun the script.\n",
    "\n",
    "Q: Do you get the same numeric results? Why? How to get some control for this variability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ebe77d",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Cross validation is a way to evaluate on the full dataset.\n",
    "The sklearn library implements the ```KFold``` function that allows to create K splits for train and test.\n",
    "\n",
    "Use it to obtain splits, plot the positive ratio in each split in the train and in the test and their difference.\n",
    "\n",
    "Q: What do you observe?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits=10\n",
    "\n",
    "kfold = 0 # TODO\n",
    "kfold_splits = [[0], [0]] # TODO\n",
    "\n",
    "\n",
    "# each split contains indices for the train and test data\n",
    "for train_indices, test_indices in kfold_splits:\n",
    "    # select the data for the fold\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    # compute ratio of positive examples\n",
    "    nb_train = 0 # TODO\n",
    "    train_positive = 0 # TODO\n",
    "    train_ratio = 0 # TODO\n",
    "    \n",
    "    nb_test = 0 # TODO\n",
    "    test_positive = 0 # TODO\n",
    "    test_ratio = 0 # TODO\n",
    "    \n",
    "    print(\"\\nTrain positive Ratio:\", train_ratio)\n",
    "    print(\"Test postivie Ratio:\", test_ratio)\n",
    "    print(\"Difference:\", train_ratio - test_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae30829b",
   "metadata": {},
   "source": [
    "One way to ensure the same positive rate in the splits is to use Stratified K Fold cross validation.\n",
    "Adapt the previous code to use ```StratifiedKFold``` and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_splits=10\n",
    "\n",
    "kfold = 0 # TODO\n",
    "kfold_splits = [[0], [0]] # TODO\n",
    "\n",
    "\n",
    "# each split contains indices for the train and test data\n",
    "for train_indices, test_indices in kfold_splits:\n",
    "    # select the data for the fold\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    # compute ratio of positive examples\n",
    "    nb_train = 0 # TODO\n",
    "    train_positive = 0 # TODO\n",
    "    train_ratio = 0 # TODO\n",
    "    \n",
    "    nb_test = 0 # TODO\n",
    "    test_positive = 0 # TODO\n",
    "    test_ratio = 0 # TODO\n",
    "    \n",
    "    print(\"\\nTrain positive Ratio:\", train_ratio)\n",
    "    print(\"Test postivie Ratio:\", test_ratio)\n",
    "    print(\"Difference:\", train_ratio - test_ratio)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926815f2",
   "metadata": {},
   "source": [
    "Q: Compare and discuss the differences between the ```KFold``` and ```StratifiedKFold``` methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
